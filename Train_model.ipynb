{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f80a1a67",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a656f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import inchi\n",
    "from models.mol2vec.features import mol2alt_sentence, mol2sentence, MolSentence, DfVec, sentences2vec\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "from statistics import mean, stdev\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "293b5019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function to handle missing values in the input data\n",
    "def mse(y_pred, y_true):\n",
    "    y_true = torch.where(torch.isnan(y_true), y_pred, y_true)\n",
    "    cost = torch.abs(y_pred - y_true)\n",
    "    cost = torch.sum(cost,dim=-1)\n",
    "    cost = cost**2\n",
    "    cost = torch.mean(cost)/2\n",
    "    return cost\n",
    "\n",
    "class CustomModel_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel_1, self).__init__()\n",
    "        self.avalon_fc1 = nn.Linear(input_dim, 2000)\n",
    "        self.avalon_fc2 = nn.Linear(2000, 700)\n",
    "        self.avalon_fc3 = nn.Linear(700, 500)\n",
    "        self.avalon_fc4 = nn.Linear(500, 100)\n",
    "\n",
    "        self.mol2vec_fc1 = nn.Linear(300,100)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.concat_layer = nn.Linear(200,59)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        avalon_x = torch.relu(self.avalon_fc1(x[:,0:1024]))\n",
    "        avalon_x = torch.relu(self.avalon_fc2(avalon_x))\n",
    "        avalon_x = torch.relu(self.avalon_fc3(avalon_x))\n",
    "        avalon_x = torch.relu(self.avalon_fc4(avalon_x))\n",
    "        \n",
    "        mol2vec_x = torch.relu(self.mol2vec_fc1(x[:,1024:]))\n",
    "        mol2vec_x = self.dropout(mol2vec_x)\n",
    "        \n",
    "        final = self.concat_layer(torch.cat((avalon_x,mol2vec_x),dim=1))\n",
    "        \n",
    "        return final\n",
    "\n",
    "class CustomModel_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel_2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1500)\n",
    "        self.fc2 = nn.Linear(1500, 500)\n",
    "        self.fc3 = nn.Linear(500, 100)\n",
    "        self.fc4 = nn.Linear(100, 59)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b41355e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23005/2230538016.py:20: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  avalon = pd.read_csv(data_path + \"Avalon_bits.csv\")\n",
      "/tmp/ipykernel_23005/2230538016.py:21: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mol2vec = pd.read_csv(data_path + \"Mol2vec.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------Fold 1-------------------------------\n",
      "Load data\n",
      "train num : 64064\n",
      "val num : 8008\n",
      "test num : 8009\n",
      "tasks: 59\n",
      "Training start\n",
      "Epoch : 0, Train loss : 2269.2004393041134, Val_loss : 136.80580353736877\n",
      "Epoch : 1, Train loss : 855.944313108921, Val_loss : 98.42281371355057\n",
      "Epoch : 2, Train loss : 653.9289003014565, Val_loss : 97.49839067459106\n",
      "Epoch : 3, Train loss : 543.1609357595444, Val_loss : 82.4841600060463\n",
      "Epoch : 4, Train loss : 487.26026195287704, Val_loss : 81.52028003334999\n",
      "Epoch : 5, Train loss : 434.9582031071186, Val_loss : 78.4399865269661\n",
      "Epoch : 6, Train loss : 408.2961251437664, Val_loss : 80.10769954323769\n",
      "Epoch : 7, Train loss : 379.38157215714455, Val_loss : 80.11142924427986\n",
      "Epoch : 8, Train loss : 381.76974272727966, Val_loss : 84.5760890841484\n",
      "Epoch : 9, Train loss : 341.64926090836525, Val_loss : 79.21719041466713\n",
      "Epoch : 10, Train loss : 326.31132151186466, Val_loss : 68.31574723124504\n",
      "Epoch : 11, Train loss : 340.31405948102474, Val_loss : 82.73247775435448\n",
      "Epoch : 12, Train loss : 303.6877055466175, Val_loss : 93.40455701947212\n",
      "Epoch : 13, Train loss : 306.8513006865978, Val_loss : 66.60961332917213\n",
      "Epoch : 14, Train loss : 293.0105072259903, Val_loss : 86.61609929800034\n",
      "Epoch : 15, Train loss : 307.2819819897413, Val_loss : 76.55152603983879\n",
      "Epoch : 16, Train loss : 316.1242425739765, Val_loss : 81.00450623035431\n",
      "Epoch : 17, Train loss : 270.17035688459873, Val_loss : 75.47633251547813\n",
      "Epoch : 18, Train loss : 245.59725484251976, Val_loss : 67.05685710906982\n",
      "Epoch : 19, Train loss : 246.20128092169762, Val_loss : 75.0367032289505\n",
      "Epoch : 20, Train loss : 226.72560060024261, Val_loss : 64.01564139127731\n",
      "Epoch : 21, Train loss : 241.99155086278915, Val_loss : 60.329453870654106\n",
      "Epoch : 22, Train loss : 242.72410213947296, Val_loss : 61.883076161146164\n",
      "Epoch : 23, Train loss : 237.62366704642773, Val_loss : 63.98001191020012\n",
      "Epoch : 24, Train loss : 225.90245348215103, Val_loss : 67.23719894886017\n",
      "Epoch : 25, Train loss : 229.15008732676506, Val_loss : 57.40436866879463\n",
      "Epoch : 26, Train loss : 212.46778598427773, Val_loss : 59.27371221780777\n",
      "Epoch : 27, Train loss : 204.4541268646717, Val_loss : 57.22912946343422\n",
      "Epoch : 28, Train loss : 212.40257981419563, Val_loss : 65.07461455464363\n",
      "Epoch : 29, Train loss : 219.18053494393826, Val_loss : 57.507941231131554\n",
      "Epoch : 30, Train loss : 207.73111534118652, Val_loss : 59.119220584630966\n",
      "Epoch : 31, Train loss : 186.41534011811018, Val_loss : 57.05054295063019\n",
      "Epoch : 32, Train loss : 174.41492791473866, Val_loss : 57.542996138334274\n",
      "Epoch : 33, Train loss : 191.72122786939144, Val_loss : 58.83167390525341\n",
      "Epoch : 34, Train loss : 186.65627966821194, Val_loss : 58.02938935160637\n",
      "Epoch : 35, Train loss : 175.65860826522112, Val_loss : 56.668168276548386\n",
      "Epoch : 36, Train loss : 167.87132295966148, Val_loss : 56.04493719339371\n",
      "Epoch : 37, Train loss : 162.94481778144836, Val_loss : 54.19226871430874\n",
      "Epoch : 38, Train loss : 175.4350295290351, Val_loss : 55.89758336544037\n",
      "Epoch : 39, Train loss : 173.16413168609142, Val_loss : 53.04239599406719\n",
      "Epoch : 40, Train loss : 157.81273514032364, Val_loss : 54.08453020453453\n",
      "Epoch : 41, Train loss : 145.5152126699686, Val_loss : 52.86511345207691\n",
      "Epoch : 42, Train loss : 137.08425865322351, Val_loss : 53.3927643597126\n",
      "Epoch : 43, Train loss : 137.13108736276627, Val_loss : 54.90001505613327\n",
      "Epoch : 44, Train loss : 162.09637336432934, Val_loss : 54.93036226928234\n",
      "Epoch : 45, Train loss : 156.11785539239645, Val_loss : 52.97972400486469\n",
      "Epoch : 46, Train loss : 139.42956241965294, Val_loss : 53.5452494174242\n",
      "Epoch : 47, Train loss : 131.10035964101553, Val_loss : 51.55012573301792\n",
      "Epoch : 48, Train loss : 121.25949173420668, Val_loss : 51.77338944375515\n",
      "Epoch : 49, Train loss : 121.33639218658209, Val_loss : 54.76428708434105\n",
      "Prediction start\n",
      "\n",
      "Overall performance\n",
      "Metric\tValue\n",
      "r^2\t0.64\n",
      "mae\t0.38\n",
      "mse\t0.29\n",
      "rmse\t0.54\n",
      "\n",
      "Task-wise performance\n",
      "Metric\tAvg\tStdev\n",
      "r^2\t0.51\t0.25\n",
      "mae\t%.2\n",
      "-------------------------------Fold 2-------------------------------\n",
      "Load data\n",
      "train num : 64065\n",
      "val num : 8008\n",
      "test num : 8008\n",
      "tasks: 59\n",
      "Training start\n",
      "Epoch : 0, Train loss : 2209.5535857975483, Val_loss : 199.24330747127533\n",
      "Epoch : 1, Train loss : 838.0240045785904, Val_loss : 151.62127369642258\n",
      "Epoch : 2, Train loss : 635.8510309755802, Val_loss : 124.95274353027344\n",
      "Epoch : 3, Train loss : 539.8014126718044, Val_loss : 123.48648655414581\n",
      "Epoch : 4, Train loss : 481.5501513630152, Val_loss : 117.00344985723495\n",
      "Epoch : 5, Train loss : 437.6252239048481, Val_loss : 117.42487543821335\n",
      "Epoch : 6, Train loss : 410.62404645979404, Val_loss : 113.74765181541443\n",
      "Epoch : 7, Train loss : 378.3270097076893, Val_loss : 104.62216407060623\n",
      "Epoch : 8, Train loss : 356.2393962740898, Val_loss : 99.06887084245682\n",
      "Epoch : 9, Train loss : 349.2714177966118, Val_loss : 109.91512322425842\n",
      "Epoch : 10, Train loss : 330.09923723340034, Val_loss : 105.49760603904724\n",
      "Epoch : 11, Train loss : 317.8205235302448, Val_loss : 115.82939237356186\n",
      "Epoch : 12, Train loss : 301.63390524685383, Val_loss : 94.89678370952606\n",
      "Epoch : 13, Train loss : 288.45697017014027, Val_loss : 90.12256687879562\n",
      "Epoch : 14, Train loss : 284.7648368924856, Val_loss : 90.57483077049255\n",
      "Epoch : 15, Train loss : 288.68041947484016, Val_loss : 67.6329887509346\n",
      "Epoch : 16, Train loss : 272.5057684779167, Val_loss : 73.51858353614807\n",
      "Epoch : 17, Train loss : 255.84520779550076, Val_loss : 63.5645357221365\n",
      "Epoch : 18, Train loss : 228.30310806632042, Val_loss : 61.78577870130539\n",
      "Epoch : 19, Train loss : 232.4243085384369, Val_loss : 62.22332492470741\n",
      "Epoch : 20, Train loss : 237.89118905365467, Val_loss : 65.77159984409809\n",
      "Epoch : 21, Train loss : 215.86892771720886, Val_loss : 62.993139773607254\n",
      "Epoch : 22, Train loss : 220.66215792298317, Val_loss : 62.7604648321867\n",
      "Epoch : 23, Train loss : 213.66718561202288, Val_loss : 69.84645220637321\n",
      "Epoch : 24, Train loss : 204.9252084493637, Val_loss : 66.94255524873734\n",
      "Epoch : 25, Train loss : 196.80236366391182, Val_loss : 74.55795827507973\n",
      "Epoch : 26, Train loss : 194.45182539522648, Val_loss : 62.67897066473961\n",
      "Epoch : 27, Train loss : 190.30495493113995, Val_loss : 60.57644097507\n",
      "Epoch : 28, Train loss : 197.7092535868287, Val_loss : 62.0011380314827\n",
      "Epoch : 29, Train loss : 182.83664473891258, Val_loss : 59.19997423887253\n",
      "Epoch : 30, Train loss : 181.09881345927715, Val_loss : 60.692198038101196\n",
      "Epoch : 31, Train loss : 165.7111860513687, Val_loss : 60.60565096139908\n",
      "Epoch : 32, Train loss : 160.21238485723734, Val_loss : 61.652132257819176\n",
      "Epoch : 33, Train loss : 154.65796882659197, Val_loss : 58.202667474746704\n",
      "Epoch : 34, Train loss : 167.06169998645782, Val_loss : 59.1555834710598\n",
      "Epoch : 35, Train loss : 167.46582199633121, Val_loss : 60.863947331905365\n",
      "Epoch : 36, Train loss : 176.4861085265875, Val_loss : 76.9136236011982\n",
      "Epoch : 37, Train loss : 180.10884857177734, Val_loss : 84.45756953954697\n",
      "Epoch : 38, Train loss : 168.94822148233652, Val_loss : 73.62842008471489\n",
      "Epoch : 39, Train loss : 152.14774584025145, Val_loss : 71.21191754937172\n",
      "Epoch : 40, Train loss : 151.26796529442072, Val_loss : 60.76070247590542\n",
      "Epoch : 41, Train loss : 147.0952191799879, Val_loss : 62.53765743970871\n",
      "Epoch : 42, Train loss : 149.55509559065104, Val_loss : 60.336073487997055\n",
      "Epoch : 43, Train loss : 148.94866262376308, Val_loss : 58.45828911662102\n",
      "Epoch : 44, Train loss : 146.8155091777444, Val_loss : 60.08452123403549\n",
      "Epoch : 45, Train loss : 139.1941085755825, Val_loss : 58.333399415016174\n",
      "Epoch : 46, Train loss : 125.80944750458002, Val_loss : 59.30619767308235\n",
      "Epoch : 47, Train loss : 118.20988835394382, Val_loss : 58.23578640818596\n",
      "Epoch : 48, Train loss : 114.46229185909033, Val_loss : 57.868186354637146\n",
      "Epoch : 49, Train loss : 113.85290154069662, Val_loss : 61.438979506492615\n",
      "Prediction start\n",
      "\n",
      "Overall performance\n",
      "Metric\tValue\n",
      "r^2\t0.64\n",
      "mae\t0.39\n",
      "mse\t0.30\n",
      "rmse\t0.55\n",
      "\n",
      "Task-wise performance\n",
      "Metric\tAvg\tStdev\n",
      "r^2\t0.41\t0.42\n",
      "mae\t%.2\n",
      "-------------------------------Fold 3-------------------------------\n",
      "Load data\n",
      "train num : 64065\n",
      "val num : 8008\n",
      "test num : 8008\n",
      "tasks: 59\n",
      "Training start\n",
      "Epoch : 0, Train loss : 2067.7278674542904, Val_loss : 125.65831363201141\n",
      "Epoch : 1, Train loss : 825.4235645830631, Val_loss : 110.98000240325928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2, Train loss : 624.8158215880394, Val_loss : 115.12195289134979\n",
      "Epoch : 3, Train loss : 536.7242231667042, Val_loss : 98.97653409838676\n",
      "Epoch : 4, Train loss : 481.3764029443264, Val_loss : 90.39678791165352\n",
      "Epoch : 5, Train loss : 462.2240078896284, Val_loss : 104.94420635700226\n",
      "Epoch : 6, Train loss : 417.2012409865856, Val_loss : 105.13534957170486\n",
      "Epoch : 7, Train loss : 397.6802727431059, Val_loss : 108.99419951438904\n",
      "Epoch : 8, Train loss : 376.54260228574276, Val_loss : 83.74887454509735\n",
      "Epoch : 9, Train loss : 391.625191539526, Val_loss : 88.05699929594994\n",
      "Epoch : 10, Train loss : 344.4292309731245, Val_loss : 69.37578082084656\n",
      "Epoch : 11, Train loss : 342.8526588231325, Val_loss : 72.36282858252525\n",
      "Epoch : 12, Train loss : 320.16066275537014, Val_loss : 65.4458821117878\n",
      "Epoch : 13, Train loss : 318.69737058877945, Val_loss : 77.29242485761642\n",
      "Epoch : 14, Train loss : 286.83479341864586, Val_loss : 66.43779090046883\n",
      "Epoch : 15, Train loss : 271.53682585060596, Val_loss : 60.8559008538723\n",
      "Epoch : 16, Train loss : 261.82450042665005, Val_loss : 62.09122085571289\n",
      "Epoch : 17, Train loss : 253.29068729281425, Val_loss : 62.42650857567787\n",
      "Epoch : 18, Train loss : 259.14344906806946, Val_loss : 72.156887114048\n",
      "Epoch : 19, Train loss : 247.43757298588753, Val_loss : 60.906932681798935\n",
      "Epoch : 20, Train loss : 250.48459957540035, Val_loss : 60.58037592470646\n",
      "Epoch : 21, Train loss : 216.9825786203146, Val_loss : 55.416829228401184\n",
      "Epoch : 22, Train loss : 212.70398257672787, Val_loss : 58.11125272512436\n",
      "Epoch : 23, Train loss : 220.01517578959465, Val_loss : 56.173845022916794\n",
      "Epoch : 24, Train loss : 212.42779394984245, Val_loss : 57.39408116042614\n",
      "Epoch : 25, Train loss : 229.20382241904736, Val_loss : 58.410784527659416\n",
      "Epoch : 26, Train loss : 196.8549671471119, Val_loss : 54.79422336816788\n",
      "Epoch : 27, Train loss : 182.68453153967857, Val_loss : 56.217377692461014\n",
      "Epoch : 28, Train loss : 192.5292485356331, Val_loss : 57.88304200768471\n",
      "Epoch : 29, Train loss : 199.8333587050438, Val_loss : 54.98714596033096\n",
      "Epoch : 30, Train loss : 174.2668074965477, Val_loss : 52.34704025089741\n",
      "Epoch : 31, Train loss : 179.13359784334898, Val_loss : 54.2118863761425\n",
      "Epoch : 32, Train loss : 172.70207130908966, Val_loss : 60.63902619481087\n",
      "Epoch : 33, Train loss : 172.9380994066596, Val_loss : 52.9775455147028\n",
      "Epoch : 34, Train loss : 173.36161666363478, Val_loss : 64.14186125993729\n",
      "Epoch : 35, Train loss : 170.39708674699068, Val_loss : 65.44664695858955\n",
      "Epoch : 36, Train loss : 170.73069308698177, Val_loss : 61.76559954881668\n",
      "Epoch : 37, Train loss : 153.60848288983107, Val_loss : 54.30787444114685\n",
      "Epoch : 38, Train loss : 160.17264296859503, Val_loss : 70.6214953660965\n",
      "Epoch : 39, Train loss : 146.12745320051908, Val_loss : 73.8694364130497\n",
      "Epoch : 40, Train loss : 150.74741619080305, Val_loss : 67.43235248327255\n",
      "Epoch : 41, Train loss : 149.759000338614, Val_loss : 79.17466300725937\n",
      "Epoch : 42, Train loss : 162.54556538909674, Val_loss : 59.98060968518257\n",
      "Epoch : 43, Train loss : 162.79629804939032, Val_loss : 62.484835624694824\n",
      "Epoch : 44, Train loss : 145.95954464375973, Val_loss : 53.01445060968399\n",
      "Epoch : 45, Train loss : 134.66894976049662, Val_loss : 49.00132820010185\n",
      "Epoch : 46, Train loss : 137.37680818140507, Val_loss : 53.65183171629906\n",
      "Epoch : 47, Train loss : 128.7623281404376, Val_loss : 49.382403552532196\n",
      "Epoch : 48, Train loss : 134.30696718394756, Val_loss : 49.337799325585365\n",
      "Epoch : 49, Train loss : 136.1192171126604, Val_loss : 50.61122353374958\n",
      "Prediction start\n",
      "\n",
      "Overall performance\n",
      "Metric\tValue\n",
      "r^2\t0.59\n",
      "mae\t0.41\n",
      "mse\t0.32\n",
      "rmse\t0.57\n",
      "\n",
      "Task-wise performance\n",
      "Metric\tAvg\tStdev\n",
      "r^2\t0.40\t0.31\n",
      "mae\t%.2\n",
      "-------------------------------Fold 4-------------------------------\n",
      "Load data\n",
      "train num : 64065\n",
      "val num : 8008\n",
      "test num : 8008\n",
      "tasks: 59\n",
      "Training start\n",
      "Epoch : 0, Train loss : 2308.592195659876, Val_loss : 125.97583875060081\n",
      "Epoch : 1, Train loss : 880.2861558794975, Val_loss : 109.87757277488708\n",
      "Epoch : 2, Train loss : 653.9596309959888, Val_loss : 104.42180252075195\n",
      "Epoch : 3, Train loss : 553.3457247316837, Val_loss : 107.23497048020363\n",
      "Epoch : 4, Train loss : 481.65509101748466, Val_loss : 107.97306913137436\n",
      "Epoch : 5, Train loss : 442.06496973335743, Val_loss : 96.77244916558266\n",
      "Epoch : 6, Train loss : 402.43066281080246, Val_loss : 90.39288452267647\n",
      "Epoch : 7, Train loss : 372.6344020664692, Val_loss : 86.72646278142929\n",
      "Epoch : 8, Train loss : 353.39745096862316, Val_loss : 90.27264329791069\n",
      "Epoch : 9, Train loss : 335.8067935705185, Val_loss : 84.79683104157448\n",
      "Epoch : 10, Train loss : 316.9811120480299, Val_loss : 75.9487672150135\n",
      "Epoch : 11, Train loss : 297.7668261528015, Val_loss : 80.56862905621529\n",
      "Epoch : 12, Train loss : 310.4964344203472, Val_loss : 74.03109908103943\n",
      "Epoch : 13, Train loss : 277.57007098197937, Val_loss : 85.45177626609802\n",
      "Epoch : 14, Train loss : 285.4541381150484, Val_loss : 76.98856484889984\n",
      "Epoch : 15, Train loss : 273.51477743685246, Val_loss : 73.53239959478378\n",
      "Epoch : 16, Train loss : 262.79279363155365, Val_loss : 79.09109809994698\n",
      "Epoch : 17, Train loss : 262.35805512964725, Val_loss : 60.98429246246815\n",
      "Epoch : 18, Train loss : 263.421872779727, Val_loss : 63.291225612163544\n",
      "Epoch : 19, Train loss : 236.634233340621, Val_loss : 62.253338038921356\n",
      "Epoch : 20, Train loss : 219.91659143567085, Val_loss : 74.92100211977959\n",
      "Epoch : 21, Train loss : 228.484513387084, Val_loss : 72.7713428735733\n",
      "Epoch : 22, Train loss : 213.3284182175994, Val_loss : 72.84667873382568\n",
      "Epoch : 23, Train loss : 217.9634399265051, Val_loss : 72.41317334771156\n",
      "Epoch : 24, Train loss : 218.86408702284098, Val_loss : 70.92931890487671\n",
      "Epoch : 25, Train loss : 217.8791572600603, Val_loss : 67.80315670371056\n",
      "Epoch : 26, Train loss : 202.5446892529726, Val_loss : 76.13799625635147\n",
      "Epoch : 27, Train loss : 200.7087653130293, Val_loss : 61.422483086586\n",
      "Epoch : 28, Train loss : 181.92498812824488, Val_loss : 64.71177899837494\n",
      "Epoch : 29, Train loss : 182.735665127635, Val_loss : 57.36876158416271\n",
      "Epoch : 30, Train loss : 182.8729131743312, Val_loss : 68.33463335037231\n",
      "Epoch : 31, Train loss : 189.660732537508, Val_loss : 65.05226409435272\n",
      "Epoch : 32, Train loss : 193.3951619118452, Val_loss : 60.00171685218811\n",
      "Epoch : 33, Train loss : 189.4713859707117, Val_loss : 60.43808463215828\n",
      "Epoch : 34, Train loss : 169.40454403311014, Val_loss : 60.64629706740379\n",
      "Epoch : 35, Train loss : 168.1617872491479, Val_loss : 60.35322421789169\n",
      "Epoch : 36, Train loss : 158.64529707282782, Val_loss : 60.782971292734146\n",
      "Epoch : 37, Train loss : 151.4405142366886, Val_loss : 54.14009642601013\n",
      "Epoch : 38, Train loss : 153.73008888959885, Val_loss : 53.21673226356506\n",
      "Epoch : 39, Train loss : 149.240697927773, Val_loss : 52.52716110646725\n",
      "Epoch : 40, Train loss : 164.85971332341433, Val_loss : 55.1631646156311\n",
      "Epoch : 41, Train loss : 189.53777336329222, Val_loss : 51.40539139509201\n",
      "Epoch : 42, Train loss : 151.70056366175413, Val_loss : 51.45758435130119\n",
      "Epoch : 43, Train loss : 144.72112890332937, Val_loss : 51.45438629388809\n",
      "Epoch : 44, Train loss : 141.10899100452662, Val_loss : 50.85809871554375\n",
      "Epoch : 45, Train loss : 140.19759709388018, Val_loss : 50.05071732401848\n",
      "Epoch : 46, Train loss : 133.67097061872482, Val_loss : 51.244420900940895\n",
      "Epoch : 47, Train loss : 129.63160095363855, Val_loss : 52.36064398288727\n",
      "Epoch : 48, Train loss : 132.93547204881907, Val_loss : 52.67019210755825\n",
      "Epoch : 49, Train loss : 128.63457246124744, Val_loss : 50.13884416222572\n",
      "Prediction start\n",
      "\n",
      "Overall performance\n",
      "Metric\tValue\n",
      "r^2\t0.63\n",
      "mae\t0.39\n",
      "mse\t0.31\n",
      "rmse\t0.55\n",
      "\n",
      "Task-wise performance\n",
      "Metric\tAvg\tStdev\n",
      "r^2\t0.47\t0.28\n",
      "mae\t%.2\n",
      "-------------------------------Fold 5-------------------------------\n",
      "Load data\n",
      "train num : 64065\n",
      "val num : 8008\n",
      "test num : 8008\n",
      "tasks: 59\n",
      "Training start\n",
      "Epoch : 0, Train loss : 2119.742449223995, Val_loss : 112.42926687002182\n",
      "Epoch : 1, Train loss : 842.067817568779, Val_loss : 89.41333648562431\n",
      "Epoch : 2, Train loss : 610.7208296954632, Val_loss : 81.35145968198776\n",
      "Epoch : 3, Train loss : 504.9041576832533, Val_loss : 78.9549258351326\n",
      "Epoch : 4, Train loss : 452.86931985616684, Val_loss : 73.27373826503754\n",
      "Epoch : 5, Train loss : 421.10361091792583, Val_loss : 69.91977897286415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6, Train loss : 385.6912610977888, Val_loss : 67.36844225227833\n",
      "Epoch : 7, Train loss : 373.54804991185665, Val_loss : 67.1430436372757\n",
      "Epoch : 8, Train loss : 350.3656442910433, Val_loss : 68.58828163146973\n",
      "Epoch : 9, Train loss : 345.2431185692549, Val_loss : 68.10438817739487\n",
      "Epoch : 10, Train loss : 350.4907045662403, Val_loss : 65.1615991294384\n",
      "Epoch : 11, Train loss : 318.1879439353943, Val_loss : 65.85319557785988\n",
      "Epoch : 12, Train loss : 304.21188339591026, Val_loss : 63.28145369887352\n",
      "Epoch : 13, Train loss : 310.60490445792675, Val_loss : 65.95252834260464\n",
      "Epoch : 14, Train loss : 270.56545862555504, Val_loss : 63.20058435201645\n",
      "Epoch : 15, Train loss : 285.7966040968895, Val_loss : 62.682000398635864\n",
      "Epoch : 16, Train loss : 262.12574565410614, Val_loss : 61.604928240180016\n",
      "Epoch : 17, Train loss : 257.52628783881664, Val_loss : 70.21283859014511\n",
      "Epoch : 18, Train loss : 248.1610263288021, Val_loss : 60.822383388876915\n",
      "Epoch : 19, Train loss : 254.3688156157732, Val_loss : 67.88989667594433\n",
      "Epoch : 20, Train loss : 239.3286435753107, Val_loss : 59.530056431889534\n",
      "Epoch : 21, Train loss : 222.92145185172558, Val_loss : 63.39820468425751\n",
      "Epoch : 22, Train loss : 227.32000727951527, Val_loss : 60.41701075434685\n",
      "Epoch : 23, Train loss : 218.87183551490307, Val_loss : 58.70741565525532\n",
      "Epoch : 24, Train loss : 226.15086828172207, Val_loss : 64.99130529165268\n",
      "Epoch : 25, Train loss : 198.93799395859241, Val_loss : 63.31189966201782\n",
      "Epoch : 26, Train loss : 205.83866865932941, Val_loss : 61.929218620061874\n",
      "Epoch : 27, Train loss : 209.19890077412128, Val_loss : 64.21854844689369\n",
      "Epoch : 28, Train loss : 194.68135711550713, Val_loss : 61.8283965587616\n",
      "Epoch : 29, Train loss : 185.29424305260181, Val_loss : 59.92412167787552\n",
      "Epoch : 30, Train loss : 185.54851059615612, Val_loss : 60.72586789727211\n",
      "Epoch : 31, Train loss : 168.2187464237213, Val_loss : 60.1068424731493\n",
      "Epoch : 32, Train loss : 171.85121531784534, Val_loss : 58.57533197104931\n",
      "Epoch : 33, Train loss : 180.2919124662876, Val_loss : 59.3205471932888\n",
      "Epoch : 34, Train loss : 175.94467674195766, Val_loss : 62.63329550623894\n",
      "Epoch : 35, Train loss : 156.46479958295822, Val_loss : 55.845338955521584\n",
      "Epoch : 36, Train loss : 165.7444595694542, Val_loss : 62.53406976163387\n",
      "Epoch : 37, Train loss : 165.03700962662697, Val_loss : 58.09596836566925\n",
      "Epoch : 38, Train loss : 152.44470832496881, Val_loss : 61.753694266080856\n",
      "Epoch : 39, Train loss : 137.77904979884624, Val_loss : 54.70376215875149\n",
      "Epoch : 40, Train loss : 146.08333157747984, Val_loss : 61.01044833660126\n",
      "Epoch : 41, Train loss : 150.4062441289425, Val_loss : 61.18492087721825\n",
      "Epoch : 42, Train loss : 147.01684517413378, Val_loss : 59.94256913661957\n",
      "Epoch : 43, Train loss : 138.05608892440796, Val_loss : 56.687176913022995\n",
      "Epoch : 44, Train loss : 142.035789296031, Val_loss : 58.80780377984047\n",
      "Epoch : 45, Train loss : 136.43962554633617, Val_loss : 61.53206068277359\n",
      "Epoch : 46, Train loss : 137.12947457283735, Val_loss : 56.963027358055115\n",
      "Epoch : 47, Train loss : 121.05679048597813, Val_loss : 61.49019484221935\n",
      "Epoch : 48, Train loss : 132.0065710991621, Val_loss : 54.24412517249584\n",
      "Epoch : 49, Train loss : 132.8515352383256, Val_loss : 61.108103439211845\n",
      "Prediction start\n",
      "\n",
      "Overall performance\n",
      "Metric\tValue\n",
      "r^2\t0.63\n",
      "mae\t0.40\n",
      "mse\t0.31\n",
      "rmse\t0.56\n",
      "\n",
      "Task-wise performance\n",
      "Metric\tAvg\tStdev\n",
      "r^2\t0.35\t0.43\n",
      "mae\t%.2\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device('cuda:6') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# 경로 설정\n",
    "path = os.getcwd()\n",
    "model_path = path + '/models/'\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "data_path = path + '/data/'\n",
    "random_split = data_path + '/random_split'\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "if not os.path.exists(random_split):\n",
    "    os.makedirs(random_split)\n",
    "\n",
    "# Load training dataset\n",
    "avalon = pd.read_csv(data_path + \"Avalon_bits.csv\")\n",
    "mol2vec = pd.read_csv(data_path + \"Mol2vec.csv\")\n",
    "ld50 = pd.read_csv(data_path + \"dataset.csv\")\n",
    "\n",
    "mol2vec_df = pd.concat([ld50.iloc[:,0:1],avalon.iloc[:,2:],mol2vec.iloc[:,2:],ld50.iloc[:,2:]],axis=1)\n",
    "\n",
    "\n",
    "# kfold start\n",
    "kfold=KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "fold = 0\n",
    "for train_indices, test_index in kfold.split(mol2vec_df):\n",
    "    validation_indices, test_indices = train_test_split(test_index,test_size = 0.5, random_state=seed)\n",
    "    fold += 1\n",
    "    print(f'-------------------------------Fold {fold}-------------------------------')\n",
    "    print('Load data')\n",
    "    print(f'train num : {len(train_indices)}')\n",
    "    print(f'val num : {len(validation_indices)}')\n",
    "    print(f'test num : {len(test_indices)}')\n",
    "    \n",
    "    tasks = 59 # task 개수\n",
    "    task_index = 1325 # model input과 task를 나누기 위한 index 값\n",
    "    print('tasks: %s' % tasks)\n",
    "    X_train = mol2vec_df.iloc[train_indices,1:task_index].values\n",
    "    y_train = mol2vec_df.iloc[train_indices,task_index:].values\n",
    "    X_val = mol2vec_df.iloc[validation_indices,1:task_index].values\n",
    "    y_val = mol2vec_df.iloc[validation_indices,task_index:].values\n",
    "    X_test = mol2vec_df.iloc[test_indices,1:task_index].values\n",
    "    y_test = mol2vec_df.iloc[test_indices,task_index:].values\n",
    "    \n",
    "    mol2vec.iloc[train_indices,0:2].to_csv(f'{random_split}' + f'/train_fold_{fold}.csv',index=False)\n",
    "    mol2vec.iloc[validation_indices,0:2].to_csv(f'{random_split}' + f'/validation_fold_{fold}.csv',index=False)\n",
    "    mol2vec.iloc[test_indices,0:2].to_csv(f'{random_split}' + f'/test_fold_{fold}.csv',index=False)\n",
    "    \n",
    "    # Load test dataset\n",
    "    df_test = pd.concat([mol2vec_df.iloc[val_indices,0:1],mol2vec_df.iloc[val_indices,task_index:]],axis=1)\n",
    "    \n",
    "    task_list = list(mol2vec_df.iloc[:,task_index:].columns)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    model = CustomModel_2().to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    criterion = mse\n",
    "\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "    \n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 50\n",
    "    batch_size = 128 \n",
    "\n",
    "    print('Training start')\n",
    "    best_val_loss = 555555555555555555\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        for i in range(0, len(X_train_tensor), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            train_batch_x, train_batch_y = X_train_tensor[i:i+batch_size], y_train_tensor[i:i+batch_size]\n",
    "            outputs = model(train_batch_x)\n",
    "            loss = criterion(outputs, train_batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        val_loss = 0\n",
    "        for i in range(0, len(X_val_tensor), batch_size):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_batch_x, val_batch_y = X_val_tensor[i:i+batch_size], y_val_tensor[i:i+batch_size]\n",
    "                val_outputs = model(val_batch_x)\n",
    "                val_loss += criterion(val_outputs,val_batch_y).item()\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), model_path + f'Best_model_fold_{fold}')    \n",
    "        print(f'Epoch : {epoch}, Train loss : {train_loss}, Val_loss : {val_loss}')\n",
    "    # Evaluation\n",
    "    print('Prediction start')\n",
    "    model = CustomModel_2()\n",
    "    model.load_state_dict(torch.load(model_path + f'Best_model_fold_{fold}'))\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_val_tensor).numpy()\n",
    "\n",
    "    # Transform predictions to DataFrame\n",
    "    df_pred = pd.DataFrame(data=y_pred, columns=task_list)\n",
    "    rtecs_ids = df_test['RTECS_ID'].values\n",
    "    df_pred = df_pred.assign(RTECS_ID=rtecs_ids)\n",
    "\n",
    "    # Reshape df_test and df_pred\n",
    "    df_test = pd.melt(df_test, id_vars='RTECS_ID', value_vars=task_list, var_name='Task', value_name='LD50')\n",
    "    df_pred = pd.melt(df_pred, id_vars='RTECS_ID', value_vars=task_list, var_name='Task', value_name='pred_LD50')\n",
    "\n",
    "    # Merge df_test and df_pred\n",
    "    final_df = pd.merge(df_test, df_pred,  how='left', left_on=['RTECS_ID','Task'], right_on = ['RTECS_ID','Task'])\n",
    "    final_df = final_df.dropna(subset=['LD50'])\n",
    "\n",
    "    final_df.to_csv(f\"/home/psy/LD50/results/mol2vec_fold_{fold}.csv\", index=False)\n",
    "\n",
    "    y_true = final_df['LD50'].values\n",
    "    y_pred = final_df['pred_LD50'].values\n",
    "\n",
    "    # Overall performance\n",
    "    print(\"\\nOverall performance\")\n",
    "    print(\"Metric\\tValue\")\n",
    "    print(\"r^2\\t%.2f\" % r2_score(y_true, y_pred))\n",
    "    print(\"mae\\t%.2f\" % mean_absolute_error(y_true, y_pred))\n",
    "    print(\"mse\\t%.2f\" % mean_squared_error(y_true, y_pred))\n",
    "    print(\"rmse\\t%.2f\" % sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "    # Calculate performance task-wise\n",
    "    r2_scores = []\n",
    "    mae_scores = []\n",
    "    mse_scores = []\n",
    "    rmse_scores = []\n",
    "\n",
    "    for task, dft in final_df.groupby('Task'):\n",
    "        y_true = dft['LD50'].values\n",
    "        y_pred = dft['pred_LD50'].values\n",
    "\n",
    "        r2_scores.append(r2_score(y_true, y_pred))\n",
    "        mae_scores.append(mean_absolute_error(y_true, y_pred))\n",
    "        mse_scores.append(mean_squared_error(y_true, y_pred))\n",
    "        rmse_scores.append(sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "    # Task-wise performance\n",
    "    print(\"\\nTask-wise performance\")\n",
    "    print(\"Metric\\tAvg\\tStdev\")\n",
    "    print(\"r^2\\t%.2f\\t%.2f\" % (mean(r2_scores), stdev(r2_scores)))\n",
    "    print(\"mae\\t%.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2868434",
   "metadata": {},
   "source": [
    "# Results of test_fold_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bd779cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Avalon import pyAvalonTools\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "from models.mol2vec.features import mol2alt_sentence, mol2sentence, MolSentence, DfVec, sentences2vec\n",
    "from models.mol2vec.helpers import depict_identifier, mol_to_svg\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "from statistics import mean, stdev\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "749caa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel_2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1500)\n",
    "        self.fc2 = nn.Linear(1500, 500)\n",
    "        self.fc3 = nn.Linear(500, 100)\n",
    "        self.fc4 = nn.Linear(100, 59)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "def toxic_label(value):\n",
    "    if value >= 5000: \n",
    "        return 'Safe_chemicals'\n",
    "    elif value >= 500:\n",
    "        return 'Slightly_toxic'\n",
    "    elif value >= 50:\n",
    "        return 'Moderately_toxic'\n",
    "    else:\n",
    "        return 'Highest_toxic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3d40674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "path = os.getcwd()\n",
    "model_path = path + '/models/'\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "data_path = path + '/data/'\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "    \n",
    "device = torch.device('cuda:6') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# reverse standardization을 위한 MolWt 추출\n",
    "data = pd.read_csv(\"/home/psy/LD50/Toxicity_prediction/data/random_split/test_fold_1.csv\")\n",
    "smiles_list = data['SMILES'].values # SMILES 부분민 추출\n",
    "mol = [ Chem.MolFromSmiles(smiles) for smiles in smiles_list]\n",
    "molecular_weight = [Descriptors.MolWt(mols) for mols in mol]\n",
    "data['MolWt'] = molecular_weight\n",
    "\n",
    "# SMILES 문자열을 Molecule 객체로 변환하고 Avalon fingerprint 생성\n",
    "avalon_fps = []\n",
    "for smiles in smiles_list: # 각각의 smile에 해당하는 avalon_fp 를 얻기 위한 for문\n",
    "    mol = Chem.MolFromSmiles(smiles) # smiles를 molecule로 변환\n",
    "    if mol is not None:\n",
    "        avalon_fp = rdkit.Avalon.pyAvalonTools.GetAvalonFP(mol, nBits=1024) # molecule 을 avalon_fp로 변환\n",
    "        binary_avalon_fp = avalon_fp.ToList() # 0,1 로 구성된 1024bit로 표현\n",
    "        avalon_fps.append(binary_avalon_fp)\n",
    "    else:\n",
    "        print(f\"Failed to generate Avalon fingerprint for SMILES: {smiles}\") # SMILES 정보가 database에 존재하지 않을 시 error\n",
    "avalon_fps = np.array(avalon_fps)\n",
    "\n",
    "# avalon_fp의 각 col name을 Avalon_i 로 표현\n",
    "col_list = [] \n",
    "for i in range(1024):\n",
    "    col_names = f'Avalon_{i}'\n",
    "    col_list.append(col_names)\n",
    "    \n",
    "avalon_bits = pd.DataFrame(avalon_fps)\n",
    "avalon_bits.columns = col_list\n",
    "\n",
    "# Unseen data의 mo2lvec embedding vector 생성\n",
    "mol2vec_df = data.iloc[:,0:2] # ID와 smiles만 추출\n",
    "\n",
    "mol = [Chem.MolFromSmiles(x) for x in smiles_list]\n",
    "#Draw.MolsToGridImage(mol, molsPerRow=5, useSVG=False) # 시각화\n",
    "mol2vec_df['ROMol'] = mol\n",
    "\n",
    "# molecule 별로 sentence 생성\n",
    "mol2vec_df['sentence'] = mol2vec_df.apply(lambda x: MolSentence(mol2alt_sentence(x['ROMol'], 1)), axis=1)\n",
    "\n",
    "# pretrained mol2vec model 불러오기\n",
    "model = word2vec.Word2Vec.load(model_path + 'mol2vec/mol2vec_300dim.pkl')\n",
    "\n",
    "# mol2vec embedding vector 생성\n",
    "mol2vec_df['mol2vec'] = [DfVec(x) for x in sentences2vec(mol2vec_df['sentence'], model, unseen='UNK')]\n",
    "\n",
    "# mol2vec embedding vector 저장\n",
    "mol2vec_emb = np.array([x.vec for x in mol2vec_df['mol2vec']])\n",
    "\n",
    "col_list = [] \n",
    "for i in range(300):\n",
    "    col_names = f'Mol2vec_{i}'\n",
    "    col_list.append(col_names)\n",
    "    \n",
    "Mol2vec = pd.DataFrame(mol2vec_emb)\n",
    "Mol2vec.columns = col_list\n",
    "\n",
    "# prediction 결과를 reverse standardization 하여 class_assign을 하고 toxicity count와 percantage, score를 구하는 코드\n",
    "# remove_col = data.columns[2:1085]\n",
    "# data = data.drop(columns=remove_col)\n",
    "\n",
    "test_set = pd.concat([data,avalon_bits,Mol2vec],axis=1)\n",
    "X_test = test_set.iloc[:,3:].values\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "input_dim = X_test.shape[1]\n",
    "\n",
    "model = CustomModel_2().to(device)\n",
    "model.load_state_dict(torch.load(model_path + f'Best_model_fold_1'))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).cpu().numpy()\n",
    "    \n",
    "# task_list = remove_col[0:59]\n",
    "pred = pd.DataFrame(y_pred)\n",
    "# pred.columns = task_list\n",
    "pred['molwt'] = data['MolWt'].values\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    reverse_standardization = (1/10**(pred.iloc[i,0:59]))*pred.iloc[i,59]*10**(3)\n",
    "    pred.iloc[i,0:59] = reverse_standardization\n",
    "    \n",
    "pred_label = pred.iloc[:,0:-1]\n",
    "pred_label = pred_label.applymap(toxic_label)\n",
    "pred_label = pd.concat([data,pred_label],axis=1)\n",
    "count_label = pred_label.iloc[:,3:].T\n",
    "\n",
    "pred_label['Safe_chemicals'] = 0\n",
    "pred_label['Slightly_toxicity'] = 0\n",
    "pred_label['Moderately_toxicity'] = 0\n",
    "pred_label['Highest_toxicity'] = 0\n",
    "\n",
    "count_list = []\n",
    "for i in range(len(count_label.columns)):\n",
    "    dict = count_label.iloc[:,i].value_counts().to_dict()\n",
    "    count_list.append(dict)\n",
    "\n",
    "for i in range(len(count_label.columns)):\n",
    "    try:\n",
    "        pred_label.loc[i,'Safe_chemicals'] = count_list[i]['Safe_chemicals']\n",
    "    except KeyError:\n",
    "        pred_label.loc[i,'Safe_chemicals'] = 0\n",
    "    try:\n",
    "        pred_label.loc[i,'Slightly_toxicity'] = count_list[i]['Slightly_toxic']\n",
    "    except KeyError:\n",
    "        pred_label.loc[i,'Slightly_toxicity'] = 0\n",
    "    try:\n",
    "        pred_label.loc[i,'Moderately_toxicity'] = count_list[i]['Moderately_toxic']\n",
    "    except KeyError:\n",
    "        pred_label.loc[i,'Moderately_toxicity'] = 0\n",
    "    try:\n",
    "        pred_label.loc[i,'Highest_toxicity'] = count_list[i]['Highest_toxic']\n",
    "    except KeyError:\n",
    "        pred_label.loc[i,'Highest_toxicity'] = 0\n",
    "        \n",
    "toxic_count = pred_label.iloc[:,-4:].values\n",
    "toxic_percentage = np.round((toxic_count/59)*100,2)\n",
    "\n",
    "col_names = ['Safe_chemicals_per','Slightly_toxicity_per','Moderately_toxicity_per','Highest_toxicity_per']\n",
    "toxic_per = pd.DataFrame(toxic_percentage,columns = col_names)\n",
    "\n",
    "results = pd.concat([pred_label.iloc[:,0:2],pred_label.iloc[:,-4:],toxic_per],axis=1)\n",
    "results['toxic_score'] = np.round((results['Safe_chemicals']*0 + results['Slightly_toxicity']*3.33 + results['Moderately_toxicity']*6.66 + results['Highest_toxicity']*10)/59,2)\n",
    "results['std'] = np.round(results.iloc[:,2:6].std(axis=1),2)\n",
    "\n",
    "results.to_csv(path+'/Results/Prediction_of_toxicity_with_test_fold_1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425311f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Healthcare",
   "language": "python",
   "name": "healthcare"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
